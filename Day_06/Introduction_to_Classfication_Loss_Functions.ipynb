{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SohailaDiab/365-Days-of-AI/blob/main/Day_06/Introduction_to_Classfication_Loss_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "861ncVuLPeyF"
      },
      "source": [
        "![image_2021-10-30_133041.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 6 Objectives: \n",
        "* To introduce you to loss functions. \n"
      ],
      "metadata": {
        "id": "5rokz_Xr0kDG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I1Y9BQxq72l"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t0A5Xui7sum"
      },
      "source": [
        "# Classification Losses\n",
        "\n",
        "In classification, the outputs are in form of a class or a category. The label or number assigned to the classes do not have a numerical meaning. \n",
        "\n",
        "For example, an input with class label 0 cannot be numerically compared with an input with class label 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERGOjkbwjCT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b17f296d-b29e-4329-830d-28e27030d785"
      },
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.cifar100.load_data(label_mode=\"fine\")\n",
        "\n",
        "training_images=training_images.reshape(50000, 32, 32, 3)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 32, 32, 3)\n",
        "test_images=test_images/255.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 13s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(training_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si3e84IuWiNv",
        "outputId": "8f1307d9-8970-484d-b9d1-dd178b8ec840"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
              "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
              "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EbtSlPYZYju"
      },
      "source": [
        "## Kullback-Leibler Divergence [KLD]\n",
        "\n",
        "Kullback Leibler Divergence Loss is a measure of how a distribution varies from a reference distribution (or a baseline distribution). A Kullback Leibler Divergence Loss of zero means that both the probability distributions are identical.\n",
        "\n",
        "The number of information lost in the predicted distribution is used as a measure.\n",
        "\n",
        "$$KLD(p||q) = \\int_x p(x) \\log \\frac{p(x)}{q(x)} dx$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkyXvdIQZZyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c88b218-e8d4-4654-e17d-0dbde1aa5f16"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='kl_divergence', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 19s 6ms/step - loss: 455.9123 - accuracy: 0.0101 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9120 - accuracy: 0.0100 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9124 - accuracy: 0.0103 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9126 - accuracy: 0.0096 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 455.9124 - accuracy: 0.0097 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 455.9121 - accuracy: 0.0101 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9122 - accuracy: 0.0100 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9122 - accuracy: 0.0100 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 455.9122 - accuracy: 0.0094 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9123 - accuracy: 0.0105 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9124 - accuracy: 0.0093 - val_loss: 455.9109 - val_accuracy: 0.0102\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 12s 7ms/step - loss: 455.9120 - accuracy: 0.0103 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 455.9124 - accuracy: 0.0100 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9121 - accuracy: 0.0094 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9123 - accuracy: 0.0104 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 455.9122 - accuracy: 0.0101 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9121 - accuracy: 0.0098 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9122 - accuracy: 0.0102 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 455.9122 - accuracy: 0.0104 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 455.9123 - accuracy: 0.0104 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 455.9120 - accuracy: 0.0096 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 455.9122 - accuracy: 0.0103 - val_loss: 455.9109 - val_accuracy: 0.0101\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 455.9120 - accuracy: 0.0101 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9121 - accuracy: 0.0100 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 455.9123 - accuracy: 0.0096 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 455.9123 - accuracy: 0.0108 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 455.9122 - accuracy: 0.0094 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9121 - accuracy: 0.0090 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9121 - accuracy: 0.0097 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 455.9121 - accuracy: 0.0100 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9119 - accuracy: 0.0102 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 455.9120 - accuracy: 0.0106 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 455.9121 - accuracy: 0.0099 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9121 - accuracy: 0.0103 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 455.9122 - accuracy: 0.0105 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9121 - accuracy: 0.0108 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 455.9125 - accuracy: 0.0094 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9119 - accuracy: 0.0105 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9124 - accuracy: 0.0104 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 455.9122 - accuracy: 0.0104 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9120 - accuracy: 0.0104 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9124 - accuracy: 0.0104 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9122 - accuracy: 0.0099 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9121 - accuracy: 0.0107 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 455.9124 - accuracy: 0.0102 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 455.9122 - accuracy: 0.0105 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9123 - accuracy: 0.0100 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 455.9124 - accuracy: 0.0102 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9121 - accuracy: 0.0098 - val_loss: 455.9109 - val_accuracy: 0.0100\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 455.9122 - accuracy: 0.0096 - val_loss: 455.9109 - val_accuracy: 0.0100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f684b138be0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIOrqUGTYjD8"
      },
      "source": [
        "##Binary Cross Entropy\n",
        "Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "Cross-entropy is also related to and often confused with logistic loss, called log loss. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably.\n",
        "\n",
        "Binary crossentropy is a loss function that is used in binary classification tasks. These are tasks that answer a question with only two choices (yes or no, A or B, 0 or 1, left or right). Several independent such questions can be answered at the same time, as in multi-label classification or in binary image segmentation. Formally, this loss is equal to the average of the categorical crossentropy loss on many two-category tasks.\n",
        "\n",
        "$$BCE = -\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot \\log(p(y_i)) + (1-y_i) \\cdot \\log(1- p(y_i))$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels = tf.reshape(tf.one_hot(training_labels, 100), [training_labels.shape[0], 100])\n",
        "print(training_labels.shape)\n",
        "\n",
        "test_labels = tf.reshape(tf.one_hot(test_labels, 100), [test_labels.shape[0], 100])\n",
        "print(test_labels.shape)"
      ],
      "metadata": {
        "id": "Ckc0jYu_LwkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b2c75c-dc44-4b2e-f742-de7ddc4e03a2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 100)\n",
            "(10000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRvhNnnJYjOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95145a30-2297-4d2d-e853-8d89d8b28231"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 11s 6ms/step - loss: 0.0601 - accuracy: 0.0358 - val_loss: 0.0507 - val_accuracy: 0.0683\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0488 - accuracy: 0.1030 - val_loss: 0.0466 - val_accuracy: 0.1397\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0454 - accuracy: 0.1595 - val_loss: 0.0443 - val_accuracy: 0.1739\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0432 - accuracy: 0.1985 - val_loss: 0.0425 - val_accuracy: 0.2113\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0414 - accuracy: 0.2288 - val_loss: 0.0413 - val_accuracy: 0.2446\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0401 - accuracy: 0.2547 - val_loss: 0.0399 - val_accuracy: 0.2622\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0391 - accuracy: 0.2750 - val_loss: 0.0395 - val_accuracy: 0.2673\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0383 - accuracy: 0.2890 - val_loss: 0.0392 - val_accuracy: 0.2744\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0376 - accuracy: 0.3015 - val_loss: 0.0378 - val_accuracy: 0.3005\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0369 - accuracy: 0.3156 - val_loss: 0.0381 - val_accuracy: 0.2979\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0364 - accuracy: 0.3234 - val_loss: 0.0378 - val_accuracy: 0.3010\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0359 - accuracy: 0.3331 - val_loss: 0.0374 - val_accuracy: 0.3092\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0354 - accuracy: 0.3413 - val_loss: 0.0374 - val_accuracy: 0.3132\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0350 - accuracy: 0.3487 - val_loss: 0.0367 - val_accuracy: 0.3221\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0346 - accuracy: 0.3545 - val_loss: 0.0361 - val_accuracy: 0.3384\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0343 - accuracy: 0.3603 - val_loss: 0.0363 - val_accuracy: 0.3285\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0341 - accuracy: 0.3646 - val_loss: 0.0361 - val_accuracy: 0.3349\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0338 - accuracy: 0.3718 - val_loss: 0.0357 - val_accuracy: 0.3433\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0336 - accuracy: 0.3782 - val_loss: 0.0359 - val_accuracy: 0.3367\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0334 - accuracy: 0.3785 - val_loss: 0.0365 - val_accuracy: 0.3272\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0331 - accuracy: 0.3841 - val_loss: 0.0361 - val_accuracy: 0.3420\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0329 - accuracy: 0.3864 - val_loss: 0.0355 - val_accuracy: 0.3443\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0328 - accuracy: 0.3914 - val_loss: 0.0357 - val_accuracy: 0.3436\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0325 - accuracy: 0.3951 - val_loss: 0.0355 - val_accuracy: 0.3473\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0324 - accuracy: 0.3984 - val_loss: 0.0359 - val_accuracy: 0.3436\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0323 - accuracy: 0.4015 - val_loss: 0.0355 - val_accuracy: 0.3467\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0321 - accuracy: 0.4083 - val_loss: 0.0362 - val_accuracy: 0.3367\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0320 - accuracy: 0.4069 - val_loss: 0.0355 - val_accuracy: 0.3463\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0318 - accuracy: 0.4110 - val_loss: 0.0358 - val_accuracy: 0.3504\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0317 - accuracy: 0.4156 - val_loss: 0.0358 - val_accuracy: 0.3449\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0315 - accuracy: 0.4185 - val_loss: 0.0354 - val_accuracy: 0.3562\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0314 - accuracy: 0.4181 - val_loss: 0.0352 - val_accuracy: 0.3552\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0313 - accuracy: 0.4195 - val_loss: 0.0355 - val_accuracy: 0.3498\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0312 - accuracy: 0.4244 - val_loss: 0.0355 - val_accuracy: 0.3536\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0311 - accuracy: 0.4224 - val_loss: 0.0355 - val_accuracy: 0.3528\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0310 - accuracy: 0.4244 - val_loss: 0.0355 - val_accuracy: 0.3535\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0309 - accuracy: 0.4269 - val_loss: 0.0360 - val_accuracy: 0.3472\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0308 - accuracy: 0.4307 - val_loss: 0.0352 - val_accuracy: 0.3533\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0307 - accuracy: 0.4294 - val_loss: 0.0359 - val_accuracy: 0.3514\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0307 - accuracy: 0.4318 - val_loss: 0.0361 - val_accuracy: 0.3430\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0306 - accuracy: 0.4354 - val_loss: 0.0354 - val_accuracy: 0.3531\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0304 - accuracy: 0.4369 - val_loss: 0.0355 - val_accuracy: 0.3530\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0304 - accuracy: 0.4388 - val_loss: 0.0357 - val_accuracy: 0.3524\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0303 - accuracy: 0.4389 - val_loss: 0.0361 - val_accuracy: 0.3472\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0302 - accuracy: 0.4422 - val_loss: 0.0359 - val_accuracy: 0.3466\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0302 - accuracy: 0.4430 - val_loss: 0.0354 - val_accuracy: 0.3527\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0300 - accuracy: 0.4457 - val_loss: 0.0355 - val_accuracy: 0.3531\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0300 - accuracy: 0.4453 - val_loss: 0.0362 - val_accuracy: 0.3443\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0300 - accuracy: 0.4458 - val_loss: 0.0359 - val_accuracy: 0.3590\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0298 - accuracy: 0.4484 - val_loss: 0.0356 - val_accuracy: 0.3547\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f68480ca910>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1\n",
        "\n",
        "Do you see any problems/errors with the above code? Please describe."
      ],
      "metadata": {
        "id": "eAAuecZfTfx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 1\n",
        "\n",
        "This is a multi-class, single-label classification problem. Meaning, we have multiple classes but they are exclusive (a data point can map to only ONE label).\n",
        "\n",
        "Therefore, the softmax activation function was used correctly here.\n",
        "\n",
        "However, it was wrong to use binary cross-entropy loss.\n",
        "Softmax outputs a probability for each class and we pick the highest probability to be the label. \n",
        "\n",
        "Using binary cross entropy here means that it will consider our problem to be multi-label as it will look at the output of the sigmoid for each class to be binary (will be considered a label if probability is above a certain threshold)"
      ],
      "metadata": {
        "id": "qH6xgkHPTmWp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDBdn-P976v9"
      },
      "source": [
        "## Categorical Cross Entropy\n",
        "\n",
        "This is the most common setting for classification problems. Cross-entropy loss increases as the **predicted probability** strays away from the **actual label**.\n",
        "\n",
        "Note that we have to compare the probabilities (e.g. [0.20, 0.75, 0.05]) of all the classes with the actual labels (e.g., [0, 1, 0]). The actual labels would be one-hot encoding.\n",
        "\n",
        "An important aspect of this is that cross entropy loss penalizes heavily the predictions that are confident but wrong.\n",
        "\n",
        "We are multiplying the log of the actual predicted probability for the ground truth class.\n",
        "\n",
        "$$CCE = -\\frac{1}{N}\\sum_{i=1}^{N}y_i\\log(\\hat{y}_i)$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uskCvsUSrR0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66612601-ff7b-4e65-da9b-3c77d2caf011"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=25, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 3.9418 - accuracy: 0.0957 - val_loss: 3.5633 - val_accuracy: 0.1575\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 3.3637 - accuracy: 0.1916 - val_loss: 3.2697 - val_accuracy: 0.2132\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 3.1034 - accuracy: 0.2411 - val_loss: 3.0547 - val_accuracy: 0.2515\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.9386 - accuracy: 0.2691 - val_loss: 2.9630 - val_accuracy: 0.2753\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.8108 - accuracy: 0.2953 - val_loss: 2.8953 - val_accuracy: 0.2860\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 2.7140 - accuracy: 0.3159 - val_loss: 2.7840 - val_accuracy: 0.3129\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.6350 - accuracy: 0.3319 - val_loss: 2.7547 - val_accuracy: 0.3107\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.5619 - accuracy: 0.3465 - val_loss: 2.7091 - val_accuracy: 0.3235\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.5052 - accuracy: 0.3568 - val_loss: 2.7025 - val_accuracy: 0.3284\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.4591 - accuracy: 0.3655 - val_loss: 2.7127 - val_accuracy: 0.3274\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.4149 - accuracy: 0.3738 - val_loss: 2.6535 - val_accuracy: 0.3313\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3740 - accuracy: 0.3823 - val_loss: 2.6316 - val_accuracy: 0.3442\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 2.3390 - accuracy: 0.3913 - val_loss: 2.6033 - val_accuracy: 0.3447\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3024 - accuracy: 0.3990 - val_loss: 2.6050 - val_accuracy: 0.3554\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.2749 - accuracy: 0.4052 - val_loss: 2.6236 - val_accuracy: 0.3465\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 2.2456 - accuracy: 0.4113 - val_loss: 2.6343 - val_accuracy: 0.3460\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 2.2195 - accuracy: 0.4162 - val_loss: 2.5922 - val_accuracy: 0.3523\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.1966 - accuracy: 0.4215 - val_loss: 2.5735 - val_accuracy: 0.3600\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.1781 - accuracy: 0.4248 - val_loss: 2.5730 - val_accuracy: 0.3625\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.1583 - accuracy: 0.4272 - val_loss: 2.6063 - val_accuracy: 0.3573\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.1348 - accuracy: 0.4360 - val_loss: 2.5540 - val_accuracy: 0.3623\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.1111 - accuracy: 0.4392 - val_loss: 2.6001 - val_accuracy: 0.3570\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 2.1048 - accuracy: 0.4408 - val_loss: 2.5340 - val_accuracy: 0.3679\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 2.0810 - accuracy: 0.4449 - val_loss: 2.6079 - val_accuracy: 0.3544\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.0666 - accuracy: 0.4497 - val_loss: 2.6149 - val_accuracy: 0.3550\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6839d89580>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2\n",
        "\n",
        "Now that you know how CCE works, you need to code it. It should give the same answer as `tf.keras.metrics.categorical_crossentropy` would."
      ],
      "metadata": {
        "id": "KYuLKOGSR4yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 2"
      ],
      "metadata": {
        "id": "TAtqr73-SLUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_crossentropy(true, pred):\n",
        "    \n",
        "    loss = sum(-(1/len(true)) * sum(true * np.log(pred)))\n",
        "\n",
        "    return loss\n",
        "\n",
        "true = tf.constant([[0.0, 1.0, 0.0],\n",
        "                    [1.0, 0.0, 0.0],\n",
        "                    [1.0, 0.0, 0.0],\n",
        "                    [0.0, 0.0, 1.0]])\n",
        "pred = tf.constant([[0.20, 0.70, 0.10],\n",
        "                    [0.80, 0.05, 0.15],\n",
        "                    [0.75, 0.10, 0.15],\n",
        "                    [0.25, 0.15, 0.60]])\n",
        "\n",
        "loss = categorical_crossentropy(true, pred)\n",
        "print(loss)\n",
        "\n",
        "print('------------------------------')\n",
        "\n",
        "loss = tf.keras.metrics.categorical_crossentropy(true, pred)\n",
        "loss = tf.reduce_mean(loss)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "mxzrT-GKSRSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e4abe3-0553-4485-bd4d-f65313715ffb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.34458154, shape=(), dtype=float32)\n",
            "------------------------------\n",
            "tf.Tensor(0.34458154, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CHuqni18OPL"
      },
      "source": [
        "## Sparse Categorical Cross Entropy\n",
        "\n",
        "Both, Categorical Cross Entropy [CCE] and Sparse Categorical Cross Entropy [SCCE] have the same loss function. The only difference is the format of $y_i$ (i.e., true labels).\n",
        "\n",
        "If $y_i$'s are one-hot encoded, we should use CCE. Examples (for a 3-class classification): [1,0,0], [0,1,0], [0,0,1]\n",
        "\n",
        "But if $y_i$'s are integers, use SCCE. Examples for above 3-class classification problem: [1], [2], [3]\n",
        "\n",
        "The usage entirely depends on how we load our dataset. One advantage of using sparse categorical cross entropy is it saves time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector.\n",
        "\n",
        "$$SCCE = -\\log(\\hat{y}_i)$$ for $i$ where $one\\text{-}hot\\text{-}encoding[i] = 1$ "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "\n",
        "training_images=training_images.reshape(50000, 32, 32, 3)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 32, 32, 3)\n",
        "test_images=test_images/255.0"
      ],
      "metadata": {
        "id": "tb6GisE4SkhX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcFK26KvCp-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3c6cc30-d18d-4faf-c109-1d29fed862e8"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 10s 5ms/step - loss: 2.5830 - accuracy: 0.2018 - val_loss: 2.3114 - val_accuracy: 0.2719\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.2131 - accuracy: 0.3131 - val_loss: 2.1460 - val_accuracy: 0.3389\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.0646 - accuracy: 0.3576 - val_loss: 2.0421 - val_accuracy: 0.3632\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9677 - accuracy: 0.3875 - val_loss: 1.9469 - val_accuracy: 0.3897\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.8871 - accuracy: 0.4097 - val_loss: 1.8990 - val_accuracy: 0.4027\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8223 - accuracy: 0.4308 - val_loss: 1.8752 - val_accuracy: 0.4161\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7723 - accuracy: 0.4465 - val_loss: 1.8317 - val_accuracy: 0.4292\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7295 - accuracy: 0.4591 - val_loss: 1.8308 - val_accuracy: 0.4333\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.6915 - accuracy: 0.4693 - val_loss: 1.7854 - val_accuracy: 0.4431\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6588 - accuracy: 0.4804 - val_loss: 1.7925 - val_accuracy: 0.4453\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6288 - accuracy: 0.4877 - val_loss: 1.8533 - val_accuracy: 0.4376\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.5987 - accuracy: 0.4953 - val_loss: 1.7426 - val_accuracy: 0.4646\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5793 - accuracy: 0.5005 - val_loss: 1.7533 - val_accuracy: 0.4611\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5596 - accuracy: 0.5063 - val_loss: 1.7603 - val_accuracy: 0.4565\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5340 - accuracy: 0.5155 - val_loss: 1.7594 - val_accuracy: 0.4582\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5141 - accuracy: 0.5207 - val_loss: 1.7405 - val_accuracy: 0.4660\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5005 - accuracy: 0.5247 - val_loss: 1.7126 - val_accuracy: 0.4688\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.4829 - accuracy: 0.5306 - val_loss: 1.7373 - val_accuracy: 0.4664\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4669 - accuracy: 0.5336 - val_loss: 1.7384 - val_accuracy: 0.4625\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4544 - accuracy: 0.5382 - val_loss: 1.7789 - val_accuracy: 0.4641\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4418 - accuracy: 0.5422 - val_loss: 1.7444 - val_accuracy: 0.4671\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4323 - accuracy: 0.5457 - val_loss: 1.7332 - val_accuracy: 0.4721\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4160 - accuracy: 0.5518 - val_loss: 1.7335 - val_accuracy: 0.4744\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4053 - accuracy: 0.5522 - val_loss: 1.7606 - val_accuracy: 0.4664\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3948 - accuracy: 0.5554 - val_loss: 1.7680 - val_accuracy: 0.4684\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3821 - accuracy: 0.5594 - val_loss: 1.7569 - val_accuracy: 0.4693\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3769 - accuracy: 0.5607 - val_loss: 1.7908 - val_accuracy: 0.4608\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3674 - accuracy: 0.5626 - val_loss: 1.7473 - val_accuracy: 0.4718\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.3535 - accuracy: 0.5667 - val_loss: 1.7397 - val_accuracy: 0.4785\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3461 - accuracy: 0.5727 - val_loss: 1.7564 - val_accuracy: 0.4716\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3353 - accuracy: 0.5721 - val_loss: 1.7273 - val_accuracy: 0.4753\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.3284 - accuracy: 0.5747 - val_loss: 1.7755 - val_accuracy: 0.4645\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.3205 - accuracy: 0.5779 - val_loss: 1.7969 - val_accuracy: 0.4675\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3108 - accuracy: 0.5785 - val_loss: 1.7661 - val_accuracy: 0.4787\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3060 - accuracy: 0.5811 - val_loss: 1.7801 - val_accuracy: 0.4771\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.2995 - accuracy: 0.5820 - val_loss: 1.7686 - val_accuracy: 0.4779\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2922 - accuracy: 0.5849 - val_loss: 1.7765 - val_accuracy: 0.4705\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2840 - accuracy: 0.5877 - val_loss: 1.7784 - val_accuracy: 0.4776\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.2777 - accuracy: 0.5892 - val_loss: 1.7936 - val_accuracy: 0.4771\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2680 - accuracy: 0.5918 - val_loss: 1.7937 - val_accuracy: 0.4737\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2652 - accuracy: 0.5909 - val_loss: 1.8528 - val_accuracy: 0.4689\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.2602 - accuracy: 0.5960 - val_loss: 1.8476 - val_accuracy: 0.4626\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2557 - accuracy: 0.5951 - val_loss: 1.8253 - val_accuracy: 0.4765\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.2475 - accuracy: 0.5992 - val_loss: 1.8417 - val_accuracy: 0.4653\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.2428 - accuracy: 0.5987 - val_loss: 1.8478 - val_accuracy: 0.4723\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2336 - accuracy: 0.6016 - val_loss: 1.8505 - val_accuracy: 0.4619\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2292 - accuracy: 0.6023 - val_loss: 1.8440 - val_accuracy: 0.4700\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2271 - accuracy: 0.6037 - val_loss: 1.9221 - val_accuracy: 0.4554\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2161 - accuracy: 0.6064 - val_loss: 1.8528 - val_accuracy: 0.4687\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2093 - accuracy: 0.6085 - val_loss: 1.8797 - val_accuracy: 0.4666\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f677ae38730>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhV-fVzcNc3p"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "What is the difference between a Multi-class and Multi-label Classification problem, and what sort of loss function would we need to learn them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k3J5QFvN0sA"
      },
      "source": [
        "### Answer 3\n",
        "\n",
        "A **multi-class classification** problem is when we have more than 2 classes, but each data point can have only ONE label. \n",
        "\n",
        "Here, a softmax activation function is used as well as **Categorical Cross-Entropy** loss function.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "A **multi-label classification problem** is when we have more than 2 classes, and the classes are NOT mutually exclusive. Meaning, a data point can have more than 1 label. \n",
        "\n",
        "Here, a **sigmoid** activation function is used as well as **Binary Cross-Entropy** loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITEN1-3-OQwd"
      },
      "source": [
        "## Question 4\n",
        "What is the relationship between Binary Cross entropy and Categorical Cross entropy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ocAQ_qUOuTK"
      },
      "source": [
        "### Answer 4\n",
        "\n",
        "They are mathematically identical for 2 classes. A 2-class Categorical Cross-Entropy would yield the same result as a 2-class Binary Cross-Entropy.\n",
        "\n",
        "Binary Cross-Entropy is a special case of Categorical Cross-Entropy for N=2\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathcal{L}(\\theta)\n",
        "&= -\\frac{1}{n}\\sum_{i=1}^n\\sum_{j=1}^m y_{ij}\\log(p_{ij}) = CCE\\\\\n",
        "&= -\\frac{1}{n}\\sum_{i=1}^n \\left[y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] = BCE\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "```py\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', ...)\n",
        "# is the same as\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', ...)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGwk3Z0xOmRk"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "What is the relationship between Sparse Cross entropy and Categorical Cross entropy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnKbybEKPdER"
      },
      "source": [
        "### Answer 5\n",
        "\n",
        "Sparse Cross-Entropy is a sparse variant of the Multi-Class Cross Entropy Loss. It has the same loss function as Categorical Cross Entropy.\n",
        "\n",
        "However, categorical Cross-Entropy is used when the labels are one-hot encoded, and Sparse Cross-Entropy is used when the labels are label (integer) encoded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RDBtUuYjuFh"
      },
      "source": [
        "# **Upload this Day 6 Colab Notebook to your Github repository under \"Day 6\" folder. Also add your *Reflection* on today's learning in README.md**"
      ]
    }
  ]
}